// src/lexer_rules.mbt (Updated according to the requested format)
// This is a single file as per your example structure.
// No `package main` in this specific file if it's meant to be directly processed by `mbt-parsergen`.

// Assume `token` and `LexError` are visible in this scope.
// If this file needs to be a module, then it should be: `package mylexer`
// and `import mylexer.token`, etc.
// For strict adherence to your single block format, I'll omit the package/import
// and assume `Token` and `LexError` are implicitly available,
// but in a real MoonBit project, they'd need to be imported.

// --- Start of the provided block format ---
{
priv enum Token {
  WHITESPACE // Explicitly include WHITESPACE as a token
  DOT        // .
  PIPE       // |
  IDENT(String) // fieldName (e.g., name, age)
  NUMBER(String) // 123, 1.23
  STRING(String) // "hello" - will include quotes
  TRUE       // true
  FALSE      // false
  NULL       // null
  LBRACKET   // [
  RBRACKET   // ]
  LBRACE     // {
  RBRACE     // }
  LPAREN     // (
  RPAREN     // )
  COMMA      // ,
  COLON      // :
  EQ         // ==
  GT         // >
  LT         // <
  GTE        // >=
  LTE        // <=
  PLUS       // +
  MINUS      // -
  STAR       // *
  SLASH      // /
  AND        // and
  OR         // or
  NOT        // not
  SELECT     // select keyword
  EOF        // End of File
  ILLEGAL    // Unrecognized token (explicitly included)
} derive(ToJson(style = "legacy"))

pub suberror LexError {
  EndOfFile
  UnexpectedEndOfFile
  Unrecognized(String)
}
}

// Main rule for lexing jq query tokens
rule token() -> Token raise LexError {
  parse {
    // 1. Whitespace: Now explicitly returned as a token
    [' ' '\t' '\r' '\n'] => { WHITESPACE }

    // 2. Multi-character operators (order matters for longest match)
    "==" => { EQ }
    ">=" => { GTE }
    "<=" => { LTE }

    // 3. Single-character operators and punctuation
    '.' => { DOT }
    '|' => { PIPE }
    '[' => { LBRACKET }
    ']' => { RBRACKET }
    '{' => { LBRACE }
    '}' => { RBRACE }
    '(' => { LPAREN }
    ')' => { RPAREN }
    ',' => { COMMA }
    ':' => { COLON }
    '>' => { GT }
    '<' => { LT }
    '+' => { PLUS }
    '-' => { MINUS }
    '*' => { STAR }
    '/' => { SLASH }

    // 4. Keywords (must be matched before generic identifiers)
    "select" => { SELECT }
    "and"    => { AND }
    "or"     => { OR }
    "not"    => { NOT }
    "true"   => { TRUE }
    "false"  => { FALSE }
    "null"   => { NULL }

    // 5. Numbers (as in your example)
    ('-'? ('0' | ['1'-'9'] ['0'-'9']* ('.' ['0'-'9']+)?) (['E' 'e'] ['+' '-']? ['0'-'9']+)?) as t => { NUMBER(t) }

    // 6. Strings (as in your example, calls lex_string to append content including quotes)
    '"' as t => {
      let buf = StringBuilder::new()
      buf.write_string(t) // Write the opening quote
      lex_string(buf, lexbuf) // lex_string will write content and closing quote
      STRING(buf.to_string())
    }

    // 7. Identifiers (must be matched after keywords)
    // Starts with a letter or underscore, followed by letters, digits, or underscores.
    (['a'-'z' 'A'-'Z' '_'] ['a'-'z' 'A'-'Z' '0'-'9' '_']*) as t => { IDENT(t) }

    // 8. End of File (explicitly returned as a token)
    "" => { EOF }

    // 9. Unrecognized characters (explicitly returned as ILLEGAL, or raise error)
    // Your example raises Unrecognized error, so I'll stick to that.
    _ as t => { raise(Unrecognized(t)) }
  }
}

// Helper rule to parse the content of a string literal, including escape sequences.
// It writes all matched characters into the provided StringBuilder, including the final '"'.
rule lex_string(buf : StringBuilder) -> Unit raise LexError {
  parse {
    '"' as t => { buf.write_string(t) } // Write the closing quote
    ( '\\' '"'
    | '\\' '\\'
    | '\\' '/'
    | '\\' 'b'
    | '\\' 'f'
    | '\\' 'n'
    | '\\' 'r'
    | '\\' 't'
    | '\\' 'u' ['0'-'9' 'A'-'F' 'a'-'f'] ['0'-'9' 'A'-'F' 'a'-'f'] ['0'-'9' 'A'-'F' 'a'-'f'] ['0'-'9' 'A'-'F' 'a'-'f']) as t => {
      buf.write_string(t)
      lex_string(buf, lexbuf)
    }
    [^ '"' '\\' '\x00'-'\x1F' '\x7f'] as t => {
      buf.write_string(t)
      lex_string(buf, lexbuf)
    }
    _ as t => { raise(Unrecognized(t)) }
    "" => { raise(UnexpectedEndOfFile) }
  }
}

{

}
